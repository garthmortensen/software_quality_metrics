```python
import sqlite3
import os
import json
import getpass
from datetime import datetime
import subprocess

class Orchestrator:
    def __init__(self, database_name, coordinated_data_path):
        self.database_name = database_name
        self.coordinated_data_path = coordinated_data_path
        self.rel_cwd = self.get_relative_cwd()
        self.table = "coordinated"
        self.next_id = self.increment_group_id()

    def get_relative_cwd(self) -> str:
        """find script directory"""
        return os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    def increment_group_id(self) -> int:
        """get the next group id as max(group_id)+1, in order to create unique counter"""
        conn = sqlite3.connect(self.database_name)
        cursor = conn.cursor()
        query = f"select distinct max(group_id) from {self.table}"
        cursor.execute(query)
        result = cursor.fetchone()[0] + 1
        conn.close()
        print(f"  incremented group_id = {result}")
        return result

    def get_user_specified_models(self) -> list:
        """read coordinated_data.json to find which models to include in the group run"""
        with open(self.coordinated_data_path, "r") as file:
            json_data = json.load(file)
        included = json_data["group_run"]["include"]
        included_models = [key for key, value in included.items() if value]
        return included_models

    def insert_queued_models(self, effective: str, model_status: str, included_models: list) -> None:
        """after pulling from coordinated_data.json, we populate coordinate table"""
        conn = sqlite3.connect(self.database_name)
        cursor = conn.cursor()
        whoami = getpass.getuser()
        today = datetime.today().strftime("%Y-%m-%d")
        for included_model in included_models:
            query = f"insert into {self.table} (group_id, date, model, effective, whoami, status) values ({self.next_id}, '{today}', '{included_model}', '{effective}', '{whoami}', '{model_status}')"
            cursor.execute(query)
        conn.commit()
        conn.close()

    def update_queued_model_as_complete(self, group_id: int, effective: str, model: str, model_status: str) -> None:
        """after pulling from setup.json, we update coordinate table"""
        conn = sqlite3.connect(self.database_name)
        cursor = conn.cursor()
        whoami = getpass.getuser()
        query = f"update {self.table} set status = '{model_status}' where group_id = {group_id} and model = '{model}'"
        cursor.execute(query)
        conn.commit()
        conn.close()

    def group_models_inserted_into_database(self) -> list:
        """select distinct models based on group_id and whoami conditions"""
        conn = sqlite3.connect(self.database_name)
        cursor = conn.cursor()
        whoami = getpass.getuser()
        query = f"select distinct model from {self.table} where group_id = {self.next_id} and whoami = '{whoami}'"
        cursor.execute(query)
        distinct_models = [row[0] for row in cursor.fetchall()]
        conn.close()
        return distinct_models

    def run_models(self):
        # Increment group_id
        included_models = self.get_user_specified_models()

        # Insert queued models into db table
        self.insert_queued_models(datetime.now().strftime("%Y%m%d%H%M%S"), "queued", included_models)

        # Get models included in run
        distinct_models = self.group_models_inserted_into_database()
        print(f"  SELECT grouped models: {distinct_models}")

        for distinct_model in distinct_models:
            conn = sqlite3.connect(self.database_name)
            cursor = conn.cursor()
            whoami = getpass.getuser()

            # Get grouped model status to ensure it hasn't run yet
            query_coord = f"select status from coordinated where model = '{distinct_model}' and effective = '{datetime.now().strftime('%Y%m%d%H%M%S')}' and whoami = '{whoami}'"
            cursor.execute(query_coord)
            row = cursor.fetchone()
            model_status = row[0] if row else None
            print(f"\n  {distinct_model}: {model_status}")

            if model_status == "completed":
                print(f"  Model {distinct_model} status: {model_status}")
            elif model_status == "queued":
                # Kick off model
                model_path = os.path.join(self.rel_cwd, "scripts", f"r_{distinct_model}.R")
                print(f"  Rscript {model_path}")
                process = subprocess.Popen(["Rscript", model_path], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, cwd=self.rel_cwd, universal_newlines=True)
                for line in process.stdout:
                    print(line.strip())
                # Update coordinate table as complete after the model finishes
                self.update_queued_model_as_complete(self.next_id, datetime.now().strftime("%Y%m%d%H%M%S"), distinct_model, "complete")
                print(f"\n  {distinct_model}: complete!")
            else:
                # Unexpected event, update status as error
                self.update_queued_model_as_complete(self.next_id, datetime.now().strftime("%Y%m%d%H%M%S"), distinct_model, "error")


if __name__ == "__main__":
    db_path = os.path.join(get_relative_cwd(), "standup_db", "costs.db")
    coordinated_data_path = os.path.join(get_relative_cwd(), "data", "coordinated", "coordinated_data.json")
    model_runner = ModelRunner(db_path, coordinated_data_path)
    model_runner.run_models()
```

```r
rm(list = ls())

# import helper functions
source("functions/setup_env.R")
source("functions/data_handling.R")


CURRENT_MODEL <- "calculator"

# handle database and required library configurations
env_settings <- yaml::read_yaml("./data/config.yaml")
setup_env(env_settings$libraries)
con_costs_db <- connect_to_database(env_settings$database$con_costs_db)

# import parameters regarding coordinated group run
coord_filename <- "./data/coordinated/coordinated_data.json"
coord_data <- jsonlite::fromJSON(coord_filename)
solo_or_group <- coord_data$execution_type
print(glue::glue("this run is: {solo_or_group}"))

# import parameters specific to model
json_filename <- glue::glue("./data/input/{CURRENT_MODEL}.json")
json_data <- jsonlite::fromJSON(json_filename)

calculate <- json_data$model_params$calculate
print(glue::glue("calculate: {calculate}"))
if (calculate) {
  # loop through all tables listed in json
  models <- json_data$model_params$models
  print(glue::glue("models: {models}"))

  # first create an empty df with date and effective timestamp columns
  query_first <- glue::glue("
    select date, effective
    from {models[[1]]}_predicted
    where effective = (select max(effective) from {models[[1]]}_predicted)
    ")
  date_effective_df <- dbGetQuery(con_costs_db, as.character(query_first))
  combined_df <- tibble(date = date_effective_df$date, effective = date_effective_df$effective)

  all_model_columns <- list()

  for (model in models) {
    query <- glue::glue("
     select date, factory_price_{model}
     from {model}_predicted
     where effective = (select max(effective) from {model}_predicted)
     ")
    print(glue::glue("model: {model}"))
    result <- dbGetQuery(con_costs_db, as.character(query))

    factory_price <- result[, 2]
    print(factory_price)
    col_name <- glue::glue("factory_price_{model}")
    print(col_name)

    # tag on the new columns
    combined_df <- add_column(combined_df, !!col_name := factory_price)

    # save for later when creating summation
    all_model_columns <- append(all_model_columns, col_name)
  }

  # sum costs across rows for total annual cost
  summed_columns <- combined_df %>%
    select(!!!all_model_columns) %>%
    rowSums()

  # Combine date column and summed columns into a tibble
  df_calculated <- tibble(date = date_effective_df$date, effective = date_effective_df$effective[1], annual_sum = summed_columns)
  print(df_calculated)

  # output to db and csv, display for verification
  table_output <- json_data$output$table
  dbWriteTable(con_costs_db, table_output, df_calculated, append = TRUE)
  write_to_csv(df_calculated, "./data/output/", table_output)
}
```


```r
rm(list = ls())

# import helper functions
source("functions/setup_env.R")
source("functions/data_handling.R")
source("functions/stats.R")

CURRENT_MODEL <- "concrete"

# handle database and required library configurations
env_settings <- yaml::read_yaml("./data/config.yaml")
setup_env(env_settings$libraries)
con_costs_db <- connect_to_database(env_settings$database$con_costs_db)

# import parameters regarding coordinated group run
coord_filename <- "./data/coordinated/coordinated_data.json"
coord_data <- jsonlite::fromJSON(coord_filename)
# solo or grouped impacts how we source effective timestamps from json or db
solo_or_group <- coord_data$execution_type
print(glue::glue("this model is: {solo_or_group}"))

# import parameters specific to model
json_filename <- glue::glue("./data/input/{CURRENT_MODEL}.json")
json_data <- jsonlite::fromJSON(json_filename)

dependent_variable <- json_data$model_params$dependent_variable

# if upstream in json, must update df_projection with newly generated column
upstream <- "upstream" %in% names(json_data$input)
print(glue::glue("this model has upstream: {upstream}"))

estimate_coefficients <- json_data$model_params$estimate_coefficients
print(glue::glue("json setup to estimate_coefficients: {estimate_coefficients}"))

predict_values <- json_data$model_params$predict_values
print(glue::glue("json setup to predict_values: {predict_values}."))

# get historical info from json
input_hist_table <- pluck(json_data, "input", "historical", "table")
input_hist_effective <- pluck(json_data, "input", "historical", "effective")
df_historical <- get_table_data(con_costs_db, input_hist_table, input_hist_effective)

# get projected info from json
input_proj_table <- pluck(json_data, "input", "projected", "table")
input_proj_effective <- pluck(json_data, "input", "projected", "effective")

if (!upstream) {
  print(glue::glue("!upstream: {!upstream}"))

  effective_projected <- pluck(json_data, "input", "projected", "effective")

  if (solo_or_group == "solo") {
    # source info from json
    effective_predicted <- format(Sys.time(), "%Y%m%d%H%M%S")
  } else if (solo_or_group == "group") {
    # source effective_projected and predicted from db
    print(glue::glue("solo_or_group: {solo_or_group}"))

    # source effective from db
    whoami <- "dog" # Sys.info()["user"]
    query_group_effective <- glue::glue("
      select
        max(group_id) as max_group_id
        , effective
      from
        (
        select *
        from coordinated
        where model = '{CURRENT_MODEL}'
          and whoami = '{whoami}'
          and status = 'queued'
        )
      ")

    # print(query_group_effective)
    df_effective <- dbGetQuery(con_costs_db, query_group_effective)
    print(glue::glue("df_effective: {df_effective}"))
    effective_predicted <- df_effective$effective
    # effective_predicted <- '19991231235959'

    # issue: when you run model which contains upstream data interactively
    # it searched db for upstream effective. instead, source effective from json
    # if (!interactive()) {
    # print("Running via Rscript")
    # effective <- dbGetQuery(con_costs_db, query_group_effective)
    # effective <- effective$effective
    # } else {
    # keep json effective value, since this is not an automated group run
    # print("Running in RStudio")
    # }
  }
  print(glue::glue("effective_projected: {effective_projected}"))
  print(glue::glue("effective_predicted: {effective_predicted}"))

  # lumber only sources effective_projected from json
  # but situationally changes where it sources predictive from, given solo or group
  df_projected <- get_table_data(con_costs_db, input_proj_table, effective_projected)
  print(glue::glue("df_projected: {df_projected}"))
}

# get upstream data
if (upstream) {
  print(glue::glue("upstream: {upstream}"))

  # update df_projected with effective_projected
  input_upstream_model <- pluck(json_data, "input", "upstream", "model")
  input_upstream_table <- pluck(json_data, "input", "upstream", "table")
  input_upstream_attribute <- pluck(json_data, "input", "upstream", "attribute")

  if (solo_or_group == "solo") {
    # source info from json
    print(glue::glue("solo_or_group: {solo_or_group}"))

    effective_projected <- pluck(json_data, "input", "upstream", "effective")
    effective_predicted <- format(Sys.time(), "%Y%m%d%H%M%S")
  } else if (solo_or_group == "group") {
    # source effective_projected and predicted from db
    print(glue::glue("solo_or_group: {solo_or_group}"))

    # source effective from db
    whoami <- "dog" # Sys.info()["user"]
    query_group_effective <- glue::glue("
      select
        max(group_id) as max_group_id
        , effective
      from
        (
        select *
        from coordinated
        where model = '{CURRENT_MODEL}'
          and whoami = '{whoami}'
          and status = 'queued'
        )
      ")

    # print(query_group_effective)

    df_effective <- dbGetQuery(con_costs_db, query_group_effective)
    print(glue::glue("df_effective: {df_effective}"))
    effective_projected <- df_effective$effective
    # effective_projected <- '19991231235959'
    effective_predicted <- effective_projected
  }
  # df_projected <- get_table_data(con_costs_db, input_upstream_table, effective_projected)
  print(glue::glue("effective_projected: {effective_projected}"))
  print(glue::glue("effective_predicted: {effective_predicted}"))

  df_projected <- get_table_data(con_costs_db, input_proj_table, input_proj_effective)
  print(glue::glue("df_projected: {df_projected}"))

  df_upstream <- get_table_data(con_costs_db, input_upstream_table, effective_projected)
  print(glue::glue("df_upstream: {df_upstream}"))

  # replacing column values
  df_projected[[input_upstream_attribute]] <- df_upstream[[input_upstream_attribute]]
  print(glue::glue("df_projected: {df_projected}"))
}

# update df_projected using json provided timestamp
if (upstream && solo_or_group == "solo") {
  print(glue::glue("upstream && solo_or_group: {upstream} && {solo_or_group}"))

  # update df_projected with effective_projected
  df_upstream <- get_table_data(con_costs_db, input_upstream_table, effective_projected)
  print(glue::glue("df_upstream: {df_upstream}"))
  df_projected[[input_upstream_attribute]] <- df_upstream[[input_upstream_attribute]]
  print(glue::glue("df_projected: {df_projected}"))
}


# get historical data and re-estimate model
variables_to_ignore <- c("id", "effective")
if (estimate_coefficients) {
  print(glue::glue("estimate_coefficients: {estimate_coefficients}"))
  model <- estimate(df_historical, dependent_variable, variables_to_ignore)
} else {
  print(glue::glue("estimate_coefficients: {estimate_coefficients}. New estimation coefficients will NOT be produced."))
}

# predict with the RDS model and write results to db with new effective
if (predict_values) {
  print(glue::glue("predict_values: {predict_values}"))

  # get model
  model <- readRDS(glue::glue("./models/{dependent_variable}.rds"))

  # predict and create formatted tibble
  df_predicted <- make_predictions(model, df_projected, dependent_variable) %>%
    mutate(effective = effective_predicted) %>% # update effective timestamp
    mutate(date = format(date, "%Y-%m-%d")) %>% # db format
    select(-id) # drop id column to satisfy unique constraint

  # write values to db and csv, then print for visual check
  output_table <- pluck(json_data, "output", "table")
  dbWriteTable(con_costs_db, output_table, df_predicted, append = TRUE)
  write_to_csv(df_predicted, "./data/output", output_table)
  display_verification(con_costs_db, output_table, dependent_variable, 3)
} else {
  print(glue::glue("predict_values: {predict_values}. New predicted values will NOT be produced."))
}

print(glue::glue("script complete: {CURRENT_MODEL}"))
```